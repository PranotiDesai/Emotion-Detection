{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMotion Detection from Text Data\n",
    "\n",
    "We have taken the dataset from the Kaggle https://www.kaggle.com/c/sa-emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM, Dense, SpatialDropout1D\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_model = KeyedVectors.load_word2vec_format('model/wiki.en/wiki.en.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='รก'\n",
    "a.encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('data/isear.csv', error_bad_lines=False,sep=\"|\")\n",
    "data =pd.DataFrame({'content':data_raw['SIT'],'sentiment':data_raw['Field2']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.groupby('sentiment').count()\n",
    "#emotions_class=['surprise','hate']#, 'worry','sadness', 'neutral','happiness']\n",
    "#data = data[data['sentiment'].isin(emotions_class)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts = {}\n",
    "def clean_text(text):\n",
    "    text= re.sub('[^a-zA-z0-9\\s]','',text)\n",
    "    stop_words_list = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    #extra replace fro removing \\\\ characters\n",
    "    words = [re.sub(\"\\\\\\\\\",'',word.lower()) for word in words if word.lower() not in stop_words_list and word.isalpha()]   #filter(lambda word: word not in stop_words_list, text.split())\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    for word in words:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] = 1\n",
    "        else:\n",
    "            word_counts[word] += 1 \n",
    "    #word_counts = word_counts_sorted\n",
    "    #text=\" \".join(words)\n",
    "    return text\n",
    "\n",
    "def create_word_index(word_count_sorted):\n",
    "    word_counts = {}\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    for wc in word_count_sorted:\n",
    "        word = wc[0]\n",
    "        word_counts[word] = wc[1] \n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            index_to_word[len(word_to_index)] = word\n",
    "    return word_counts, word_to_index, index_to_word\n",
    "\n",
    "def feature_mapping(texts, word_to_index, word_counts):\n",
    "    feature_vectors=np.zeros((len(texts),len(word_to_index)))\n",
    "    i=0\n",
    "    for text in texts.values:\n",
    "        words = nltk.word_tokenize(text)\n",
    "        for word in words:\n",
    "            if word in word_to_index:\n",
    "                feature_vectors[i,word_to_index[word]]=word_counts[word]\n",
    "        i+=1\n",
    "    return feature_vectors \n",
    "\n",
    "def label_encoder(labels):\n",
    "    \"\"\"\n",
    "    This method fits a label encoder on the string labels.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    return label_encoder.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['content']=data['content'].apply(clean_text)\n",
    "word_counts_sorted = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_sorted = word_counts_sorted[0:3500]\n",
    "word_counts, word_to_index, index_to_word = create_word_index(word_counts_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feature_vectors = feature_mapping(data['content'],word_to_index,word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7666, 3500)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_encoder(data['sentiment'])\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(feature_vectors, labels, test_size = 0.2, random_state = 36)\n",
    "#clf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30182529335071706"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, Y_train)\n",
    "pred = clf.predict(X_valid) \n",
    "clf.score(X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.79      0.85       164\n",
      "           1       0.70      1.00      0.82       301\n",
      "           2       0.73      0.99      0.84      1181\n",
      "           3       0.99      0.66      0.79      3132\n",
      "           4       0.72      0.99      0.83      1354\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6132\n",
      "   macro avg       0.81      0.89      0.83      6132\n",
      "weighted avg       0.86      0.82      0.81      6132\n",
      "\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kernel = 'rbf'\n",
    "clf = svm.SVC(kernel=kernel, gamma=1.5)\n",
    "clf.fit(X_train[0:4500], Y_train[0:4500])\n",
    "y_pred_train = clf.predict(X_train)\n",
    "print(classification_report(y_pred_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_num_words=5000\n",
    "feature_vector = map_to_features(max_num_words, data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "label_encoder = fit_label_indexer(data['sentiment'])\n",
    "labels = label_indexing(label_encoder, data['sentiment'])\n",
    "one_hot_encoder = fit_one_hot_encoder(labels)\n",
    "labels = one_hot_encode(one_hot_encoder, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(feature_vector,labels, test_size = 0.2, random_state = 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 128)          640000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 13)                2613      \n",
      "=================================================================\n",
      "Total params: 905,813\n",
      "Trainable params: 905,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_size = 200\n",
    "feature_vector_size = feature_vector.shape[1]\n",
    "batch_size=32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_num_words, embed_dim, input_length=feature_vector_size))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(lstm_size, dropout = 0.6, recurrent_dropout = 0.6))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/100\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 2.2034 - acc: 0.2391 - val_loss: 2.0902 - val_acc: 0.2622\n",
      "Epoch 2/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 2.0992 - acc: 0.2703 - val_loss: 2.0761 - val_acc: 0.2777\n",
      "Epoch 3/100\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 2.0792 - acc: 0.2811 - val_loss: 2.0599 - val_acc: 0.2858\n",
      "Epoch 4/100\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 2.0571 - acc: 0.2938 - val_loss: 2.0273 - val_acc: 0.2902\n",
      "Epoch 5/100\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 2.0264 - acc: 0.3020 - val_loss: 1.9901 - val_acc: 0.3028\n",
      "Epoch 6/100\n",
      "24000/24000 [==============================] - 40s 2ms/step - loss: 1.9946 - acc: 0.3104 - val_loss: 1.9697 - val_acc: 0.3087\n",
      "Epoch 7/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 1.9745 - acc: 0.3192 - val_loss: 1.9573 - val_acc: 0.3185\n",
      "Epoch 8/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.9612 - acc: 0.3262 - val_loss: 1.9601 - val_acc: 0.3178\n",
      "Epoch 9/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.9440 - acc: 0.3289 - val_loss: 1.9398 - val_acc: 0.3237\n",
      "Epoch 10/100\n",
      "24000/24000 [==============================] - 43s 2ms/step - loss: 1.9333 - acc: 0.3328 - val_loss: 1.9338 - val_acc: 0.3220\n",
      "Epoch 11/100\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 1.9211 - acc: 0.3392 - val_loss: 1.9278 - val_acc: 0.3268\n",
      "Epoch 12/100\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 1.9149 - acc: 0.3424 - val_loss: 1.9224 - val_acc: 0.3293\n",
      "Epoch 13/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.9108 - acc: 0.3427 - val_loss: 1.9208 - val_acc: 0.3290\n",
      "Epoch 14/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.9033 - acc: 0.3439 - val_loss: 1.9190 - val_acc: 0.3305\n",
      "Epoch 15/100\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 1.9007 - acc: 0.3450 - val_loss: 1.9167 - val_acc: 0.3390\n",
      "Epoch 16/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.8942 - acc: 0.3507 - val_loss: 1.9201 - val_acc: 0.3313\n",
      "Epoch 17/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.8944 - acc: 0.3503 - val_loss: 1.9105 - val_acc: 0.3333\n",
      "Epoch 18/100\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 1.8882 - acc: 0.3530 - val_loss: 1.9101 - val_acc: 0.3377\n",
      "Epoch 19/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.8856 - acc: 0.3490 - val_loss: 1.9114 - val_acc: 0.3350\n",
      "Epoch 20/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.8840 - acc: 0.3525 - val_loss: 1.9092 - val_acc: 0.3397\n",
      "Epoch 21/100\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 1.8805 - acc: 0.3513 - val_loss: 1.9106 - val_acc: 0.3370\n",
      "Epoch 22/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.8776 - acc: 0.3516 - val_loss: 1.9097 - val_acc: 0.3400\n",
      "Epoch 23/100\n",
      "24000/24000 [==============================] - 40s 2ms/step - loss: 1.8750 - acc: 0.3564 - val_loss: 1.9086 - val_acc: 0.3355\n",
      "Epoch 24/100\n",
      "24000/24000 [==============================] - 41s 2ms/step - loss: 1.8746 - acc: 0.3542 - val_loss: 1.9054 - val_acc: 0.3393\n",
      "Epoch 25/100\n",
      "24000/24000 [==============================] - 39s 2ms/step - loss: 1.8720 - acc: 0.3586 - val_loss: 1.9079 - val_acc: 0.3345\n",
      "Epoch 26/100\n",
      "24000/24000 [==============================] - 36s 2ms/step - loss: 1.8683 - acc: 0.3549 - val_loss: 1.9113 - val_acc: 0.3392\n",
      "Epoch 27/100\n",
      "24000/24000 [==============================] - 38s 2ms/step - loss: 1.8638 - acc: 0.3563 - val_loss: 1.9113 - val_acc: 0.3320\n",
      "Epoch 28/100\n",
      "24000/24000 [==============================] - 40s 2ms/step - loss: 1.8665 - acc: 0.3573 - val_loss: 1.9081 - val_acc: 0.3387\n",
      "Epoch 29/100\n",
      "24000/24000 [==============================] - 37s 2ms/step - loss: 1.8615 - acc: 0.3594 - val_loss: 1.9103 - val_acc: 0.3385\n",
      "Epoch 30/100\n",
      "24000/24000 [==============================] - 40s 2ms/step - loss: 1.8610 - acc: 0.3599 - val_loss: 1.9087 - val_acc: 0.3385\n",
      "Epoch 31/100\n",
      "24000/24000 [==============================] - 40s 2ms/step - loss: 1.8573 - acc: 0.3631 - val_loss: 1.9091 - val_acc: 0.3348\n",
      "Epoch 32/100\n",
      " 7168/24000 [=======>......................] - ETA: 26s - loss: 1.8662 - acc: 0.3574"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-556d3606e818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), batch_size =512, epochs = 100,  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
